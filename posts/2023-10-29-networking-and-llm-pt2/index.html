<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Networking and LLM in the Age of AI — Pt. II: Exploring Basic Interactions | Blogs on AI and Networking</title>
<meta name="keywords" content="">
<meta name="description" content="Instructions, Context, Function Calling and Prompts
The most crucial information we must provide to any LLM chat model are the context, detailed instructions, and precise prompts.
This article is part of a series — You can find Part I here
Instructions Instructions play a crucial role in shaping an LLM into a valuable assistant. Their purpose is to set clear expectations and guide narratives and context, thereby reducing inaccuracies and ensuring more precise responses.">
<meta name="author" content="">
<link rel="canonical" href="http://zhangineer.net/posts/2023-10-29-networking-and-llm-pt2/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css" integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://zhangineer.net/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://zhangineer.net/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://zhangineer.net/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://zhangineer.net/apple-touch-icon.png">
<link rel="mask-icon" href="http://zhangineer.net/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://zhangineer.net/posts/2023-10-29-networking-and-llm-pt2/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  

<meta property="og:title" content="Networking and LLM in the Age of AI — Pt. II: Exploring Basic Interactions" />
<meta property="og:description" content="Instructions, Context, Function Calling and Prompts
The most crucial information we must provide to any LLM chat model are the context, detailed instructions, and precise prompts.
This article is part of a series — You can find Part I here
Instructions Instructions play a crucial role in shaping an LLM into a valuable assistant. Their purpose is to set clear expectations and guide narratives and context, thereby reducing inaccuracies and ensuring more precise responses." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://zhangineer.net/posts/2023-10-29-networking-and-llm-pt2/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-10-29T20:16:15-04:00" />
<meta property="article:modified_time" content="2023-10-29T20:16:15-04:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Networking and LLM in the Age of AI — Pt. II: Exploring Basic Interactions"/>
<meta name="twitter:description" content="Instructions, Context, Function Calling and Prompts
The most crucial information we must provide to any LLM chat model are the context, detailed instructions, and precise prompts.
This article is part of a series — You can find Part I here
Instructions Instructions play a crucial role in shaping an LLM into a valuable assistant. Their purpose is to set clear expectations and guide narratives and context, thereby reducing inaccuracies and ensuring more precise responses."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://zhangineer.net/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Networking and LLM in the Age of AI — Pt. II: Exploring Basic Interactions",
      "item": "http://zhangineer.net/posts/2023-10-29-networking-and-llm-pt2/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Networking and LLM in the Age of AI — Pt. II: Exploring Basic Interactions",
  "name": "Networking and LLM in the Age of AI — Pt. II: Exploring Basic Interactions",
  "description": "Instructions, Context, Function Calling and Prompts\nThe most crucial information we must provide to any LLM chat model are the context, detailed instructions, and precise prompts.\nThis article is part of a series — You can find Part I here\nInstructions Instructions play a crucial role in shaping an LLM into a valuable assistant. Their purpose is to set clear expectations and guide narratives and context, thereby reducing inaccuracies and ensuring more precise responses.",
  "keywords": [
    
  ],
  "articleBody": "Instructions, Context, Function Calling and Prompts\nThe most crucial information we must provide to any LLM chat model are the context, detailed instructions, and precise prompts.\nThis article is part of a series — You can find Part I here\nInstructions Instructions play a crucial role in shaping an LLM into a valuable assistant. Their purpose is to set clear expectations and guide narratives and context, thereby reducing inaccuracies and ensuring more precise responses.\nCreating practical instructions often involves trial and error and is an iterative process. This is akin to conveying domain expert knowledge to others who aren’t familiar with the same domain. Experts familiar with their field might overlook mentioning basic facts or best practices that have become second nature to them.\nWhen it comes to network management, we should establish some fundamental expectations for the LLM:\nInsist on factual responses, avoiding any fabricated or imaginary configurations. Require precise API calls for acquiring specific information or implementing the correct configurations. Avoid making assumptions about input parameters when making API calls. If a prompt is ambiguous, the model should always seek clarification from the user. For instance, we should not assume the use of default values when deploying ACI policies. Adhere strictly to user-defined output formats. Here’s an illustrative example of global instructions:\n\"\"\" You are a co-pilot to the network engineers. All tasks are executed via function calls Only use the functions you have been provided with. Don't make assumptions about what values to plug into functions. Always ask for clarification if a user request is ambiguous when you encounter an error during a function call, pass back the exact error message, and do not interpret it \"\"\" It sets the context for networking administration and specifies the exclusive use of provided function calls. It also instructs ChatGPT not to interpret any error messages, leaving the diagnosis to the network admin. We can also define more granular instructions at the function level (more on this later). This seemingly small set of instructions was created via trial and error over several iterations.\nContext Context is similar to memory for LLM. An LLM has to be mindful of the conversation’s context to respond appropriately. Initial contexts are defined as part of the global instructions as shown above.\nAs we engage in a dialogue with ChatGPT, each new piece of information enriches the context. The entirety of the conversation history is repeatedly passed to ChatGPT as the conversation continues, enabling it to generate informed and relevant responses based on this accumulated context.\nWith that said, it’s important to note that the context in LLMs is confined to a single conversation and is restricted by the size of its memory, often referred to as the ‘Context Window. This implies that the volume of text an LLM can process at a time is limited. Therefore, “reading a book” is a challenging task for most LLMs. Don’t try sending “TCP/IP Volume 1 and 2” to ChatGPT and expect some in-depth Q\u0026A sessions.\nFunction Calling While our usual interactions with ChatGPT are conversational, yielding responses in human-readable plain text, we can also use APIs to request that ChatGPT provides a specific function with arguments. This process involves two main components:\nA pre-defined data structure For instance,\nget_fabric_health_function = create_function_config( name=\"get_fabric_health\", description=\"Get the latest fabric health, provide user min, max and average for the past 5 minutes\", properties={}, required=[] ) name: The name of the function to be called.\ndescription: This serves as an instruction. For instance, in the above example, we specify that the function should get the minimum, maximum, and average values for the past 5 minutes. ChatGPT may default to providing a single value without this explicit instruction.\nproperties: Here, we detail the specifics of the arguments. This includes providing a function description akin to an instruction but at the argument level. Additionally, we can define an enumeration parameter - a list of acceptable values for an argument. (This will be discussed in more detail in the following article.)\nrequired: This field informs ChatGPT which argument(s) are essential. If a user fails to provide sufficient information, ChatGPT may hallucinate from training data.\nNote: ChatGPT utilizes a dictionary structure to define functions. The code mentioned above is designed for abstraction purposes, which accounts for the slight differences in formatting.\nInclude defined functions in the API call res = openai.ChatCompletion.create( model=self.model, messages=self.messages, functions=self.functions, function_call=self.function_call, temperature=0 ) In the process of invoking the ChatCompletion Python API, we specifically incorporate two fields in addition to the common ones: functions`` and function_call``\nThe functions field is a list of function definitions, as previously defined. In our demonstrations, we will include all the specified functions.\nThe function_call field enables us to force the model to use a specific function. By default, this value is set to auto\ntemperature=0 field is not particular to the function call. Setting to 0 helps control hallucination\nYou can find a more detailed example with explanations on OpenAI’s Github page - How_to_call_functions_with_chat_models\nPrompts Prompts are the means through which we frame our intentions. In interactions with an LLM model, it is crucial to articulate our intentions clearly and include all relevant information within the prompt.\nEmphasizing simplicity and precision, and with a well-structured context, prompts in this scenario are analogous to CLI commands expressed in human language (Human Language Interface).\nWhile numerous strategies exist for effective prompt engineering, we aim to engage in straightforward English dialogue without relying on advanced prompt engineering yet still execute precise API calls to the APIC.\nConsequently, a basic understanding of networking is necessary to provide ChatGPT with the required parameters during the prompting process\nBelow are some example prompts. Note that here, we used ACI-specific terminology such as “fabric,” “UR (unicast routing),” and “BD (Bridge Domain)” to assess ChatGPT’s awareness of context.\n# This question intends to get the fabric health score - How is my fabric doing? # The intention is to get a list of routed BDs, including the default ones. # Excluding default BDs requires further instruction tuning, though - Can you get me a list of BDs with UR enabled? # We'll further analyze this one in the following article # Can you spot any potential issues in this prompt, though? - Can you add a new BD named VLAN5_BD to Tenant customera? # A simple question to count BDs. # We'll further analyze ChatGPT's response in the following article - How many BDs are there? Bring Everything Together Now that we’ve covered all the fundamentals, let’s see how to integrate these elements sequentially to create a streamlined process.\nOnce we receive a prompt from the user, the first step involves sending the following information to the ChatGPT API.\nGlobal instructions List of functions User prompts This results in the API returning the name of the function we should call and the necessary arguments. We then use this information to execute the appropriate API call to ACI, which yields a JSON response.\nAt this juncture, we have two options:\nWe can send the JSON response back to ChatGPT for parsing, as illustrated above. We’ll use this approach for all future demonstrations.\nAlternatively, we can parse the JSON using another function. This approach is less token-intensive but requires more significant software engineering effort.\nFinally, we present the output to the user.\nA Simple Demonstration Now that we understand the entire process flow let’s walk through the first query as a simple demonstration. (we’ll go into depth in the next article)\nQ: How is my fabric doing?\nRecall that the intention is to simply get the health score (as defined in the function definition).\nHow can I assist you today? =\u003e how's my fabric doing Making function call.... { \"name\": \"get_fabric_health\", \"arguments\": \"{}\" } In this instance, specifying the entire “Cisco ACI Fabric” context wasn’t necessary.\nThe function get_fabric_health is correctly returned from ChatGPT with no arguments.\nWe received the below output after sending the above API call to ACI.\n{ \"fabricOverallHealthHist5min\": { \"attributes\": { \"childAction\": \"\", \"cnt\": \"30\", \"dn\": \"topology/HDfabricOverallHealth5min-0\", \"healthAvg\": \"81\", \"healthMax\": \"81\", \"healthMin\": \"81\", \"healthSpct\": \"0\", \"healthThr\": \"\", \"healthTr\": \"0\", \"index\": \"0\", \"lastCollOffset\": \"300\", \"repIntvEnd\": \"2023-10-17T19:43:56.439+00:00\", \"repIntvStart\": \"2023-10-17T19:38:55.934+00:00\", \"status\": \"\" } } } The JSON output is subsequently sent to ChatGPT for additional data extraction. Following this, ChatGPT provided a response that precisely captured the requested average, minimum, and maximum values despite them being identical\nThe fabric health for the past 5 minutes has been consistent with an average, minimum, and maximum health score of 81. Variations in Prompts Let’s examine how ChatGPT responds to slight variations in phrasing the exact prompt.\nExample Prompt: what's my health score? and give me the hs\nHow can I assist you today? =\u003e what's my health score ? ==Response== Making function call.... { \"name\": \"get_fabric_health\", \"arguments\": \"{}\" } Your health score for the past 5 minutes is as follows: - Minimum: 84 - Maximum: 84 - Average: 84 ==End of response== How can I assist you today? =\u003e give me the hs ==Response== Making function call.... { \"name\": \"get_fabric_health\", \"arguments\": \"{}\" } Your health score for the past 5 minutes is as follows: - Minimum: 84 - Maximum: 84 - Average: 84 ==End of response== In both cases, the response outputs differed from the previous instance. This is a common scenario where asking ChatGPT the same question multiple times can result in slightly different outputs, though they convey the same meaning. In this context, the variation isn’t a significant issue. With some additional fine-tuning of the instructions, we can address this.\nRemarkably, ChatGPT correctly interpreted hs as health score. A cautionary note: only four functions were provided in this example, which probably helped control hallucinations.\nConclusion ChatGPT demonstrates proficiency in executing API actions when provided with the correct context, clear guidance, and precise function definitions.\nHowever, achieving output consistency comparable to tools like Ansible requires further refinement of instructions and consideration of user experience.\nIn the following article, we’ll analyze ChatGPT’s responses to the remaining questions in depth, examining its capability to handle more complex scenarios.\nSee you in the next one! Networking and LLM in the Age of AI - Pt III: In-Depth Analysis of ChatGPT’s Responses\n",
  "wordCount" : "1692",
  "inLanguage": "en",
  "datePublished": "2023-10-29T20:16:15-04:00",
  "dateModified": "2023-10-29T20:16:15-04:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://zhangineer.net/posts/2023-10-29-networking-and-llm-pt2/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Blogs on AI and Networking",
    "logo": {
      "@type": "ImageObject",
      "url": "http://zhangineer.net/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://zhangineer.net/" accesskey="h" title="Blogs on AI and Networking (Alt + H)">Blogs on AI and Networking</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://zhangineer.net/archives/" title="archives">
                    <span>archives</span>
                </a>
            </li>
            <li>
                <a href="http://zhangineer.net/aboutme/" title="about me">
                    <span>about me</span>
                </a>
            </li>
            <li>
                <a href="https://youtu.be/-4h7-WVhrMc?si=B4gZunEYwVa_e1UK" title="Project Demo">
                    <span>Project Demo</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://zhangineer.net/">Home</a>&nbsp;»&nbsp;<a href="http://zhangineer.net/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Networking and LLM in the Age of AI — Pt. II: Exploring Basic Interactions
    </h1>
    <div class="post-meta"><span title='2023-10-29 20:16:15 -0400 EDT'>October 29, 2023</span>

</div>
  </header> 
  <div class="post-content"><p><em>Instructions, Context, Function Calling and Prompts</em></p>
<blockquote>
<p><strong>The most crucial information we must provide to any LLM chat model are the context, detailed instructions, and precise prompts.</strong></p>
</blockquote>
<p>This article is part of a series — You can find Part I <a href="../2023-10-07-Networking-and-LLM-pt1/">here</a></p>
<p><img loading="lazy" src="banner.png" alt="banner"  />
</p>
<h2 id="instructions">Instructions<a hidden class="anchor" aria-hidden="true" href="#instructions">#</a></h2>
<p>Instructions play a crucial role in shaping an LLM into a valuable assistant. Their purpose is to set clear expectations and guide narratives and context, thereby reducing inaccuracies and ensuring more precise responses.</p>
<p>Creating practical instructions often involves trial and error and is an iterative process. This is akin to conveying domain expert knowledge to others who aren&rsquo;t familiar with the same domain. Experts familiar with their field might overlook mentioning basic facts or best practices that have become second nature to them.</p>
<p>When it comes to network management, we should establish some fundamental expectations for the LLM:</p>
<ul>
<li>Insist on factual responses, avoiding any fabricated or imaginary configurations.</li>
<li>Require precise API calls for acquiring specific information or implementing the correct configurations.</li>
<li>Avoid making assumptions about input parameters when making API calls. If a prompt is ambiguous, the model should always seek clarification from the user.
<ul>
<li>For instance, we should not assume the use of <code>default</code> values when deploying ACI policies.</li>
</ul>
</li>
<li>Adhere strictly to user-defined output formats.</li>
</ul>
<p>Here&rsquo;s an illustrative example of global instructions:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>&#34;&#34;&#34;
</span></span><span style="display:flex;"><span>You are a co-pilot to the network engineers.
</span></span><span style="display:flex;"><span>All tasks are executed via function calls
</span></span><span style="display:flex;"><span>Only use the functions you have been provided with.
</span></span><span style="display:flex;"><span>Don&#39;t make assumptions about what values to plug into functions. Always ask for clarification if a user request is ambiguous
</span></span><span style="display:flex;"><span>when you encounter an error during a function call, pass back the exact error message, and do not interpret it
</span></span><span style="display:flex;"><span>&#34;&#34;&#34;
</span></span></code></pre></div><ul>
<li>It sets the context for networking administration and specifies the exclusive use of provided function calls.</li>
<li>It also instructs ChatGPT not to interpret any error messages, leaving the diagnosis to the network admin.</li>
<li>We can also define more granular instructions at the function level (more on this later).</li>
</ul>
<blockquote>
<p><em>This seemingly small set of instructions was created via trial and error over several iterations.</em></p>
</blockquote>
<h2 id="context">Context<a hidden class="anchor" aria-hidden="true" href="#context">#</a></h2>
<p>Context is similar to memory for LLM. An LLM has to be mindful of the conversation&rsquo;s context to respond appropriately. Initial contexts are  defined as part of the global instructions as shown above.</p>
<p>As we engage in a dialogue with ChatGPT, each new piece of information enriches the context. The entirety of the conversation history is repeatedly passed to ChatGPT as the conversation continues, enabling it to generate informed and relevant responses based on this accumulated context.</p>
<p>With that said, it&rsquo;s important to note that the context in LLMs is confined to a single conversation and is restricted by the size of its memory, often referred to as the &lsquo;Context Window. This implies that the volume of text an LLM can process at a time is limited. Therefore, &ldquo;reading a book&rdquo; is a challenging task for most LLMs. Don&rsquo;t try sending &ldquo;TCP/IP Volume 1 and 2&rdquo; to ChatGPT and expect some in-depth Q&amp;A sessions.</p>
<h2 id="function-calling">Function Calling<a hidden class="anchor" aria-hidden="true" href="#function-calling">#</a></h2>
<p>While our usual interactions with ChatGPT are conversational, yielding responses in human-readable plain text, we can also use APIs to request that ChatGPT provides a specific function with arguments. This process involves two main components:</p>
<h3 id="a-pre-defined-data-structure">A pre-defined data structure<a hidden class="anchor" aria-hidden="true" href="#a-pre-defined-data-structure">#</a></h3>
<p>For instance,</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>get_fabric_health_function = create_function_config(
</span></span><span style="display:flex;"><span>    name=&#34;get_fabric_health&#34;,
</span></span><span style="display:flex;"><span>    description=&#34;Get the latest fabric health, provide user min, max and average for the past 5 minutes&#34;,
</span></span><span style="display:flex;"><span>    properties={},
</span></span><span style="display:flex;"><span>    required=[]
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><ul>
<li>
<p>name: The name of the function to be called.</p>
</li>
<li>
<p>description: This serves as an instruction. For instance, in the above example, we specify that the function should get the minimum, maximum, and average values for the past 5 minutes. ChatGPT may default to providing a single value without this explicit instruction.</p>
</li>
<li>
<p>properties: Here, we detail the specifics of the arguments. This includes providing a function description akin to an instruction but at the <em>argument level</em>. Additionally, we can define an <em>enumeration parameter</em>  -  a list of acceptable values for an argument. (This will be discussed in more detail in the following article.)</p>
</li>
<li>
<p>required: This field informs ChatGPT which argument(s) are essential. If a user fails to provide sufficient information, ChatGPT may hallucinate from training data.</p>
</li>
</ul>
<p><strong>Note</strong>: ChatGPT utilizes a dictionary structure to define functions. The code mentioned above is designed for abstraction purposes, which accounts for the slight differences in formatting.</p>
<h3 id="include-defined-functions-in-the-apicall">Include defined functions in the API call<a hidden class="anchor" aria-hidden="true" href="#include-defined-functions-in-the-apicall">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>res = openai.ChatCompletion.create(
</span></span><span style="display:flex;"><span>    model=self.model,
</span></span><span style="display:flex;"><span>    messages=self.messages,
</span></span><span style="display:flex;"><span>    functions=self.functions,
</span></span><span style="display:flex;"><span>    function_call=self.function_call,
</span></span><span style="display:flex;"><span>    temperature=0
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><ul>
<li>
<p>In the process of invoking the ChatCompletion Python API, we specifically incorporate two fields in addition to the common ones: <code>functions`` and </code>function_call``</p>
</li>
<li>
<p>The <code>functions</code> field is a list of function definitions, as previously defined. In our demonstrations, we will include all the specified functions.</p>
</li>
<li>
<p>The <code>function_call</code> field enables us to force the model to use a specific function. By default, this value is set to <code>auto</code></p>
</li>
<li>
<p><code>temperature=0</code> field is not particular to the function call. Setting to <code>0</code> helps control hallucination</p>
</li>
<li>
<p>You can find a more detailed example with explanations on OpenAI&rsquo;s Github page - <a href="https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_with_chat_models.ipynb">How_to_call_functions_with_chat_models</a></p>
</li>
</ul>
<h2 id="prompts">Prompts<a hidden class="anchor" aria-hidden="true" href="#prompts">#</a></h2>
<p>Prompts are the means through which we frame our intentions. In interactions with an LLM model, it is crucial to articulate our intentions clearly and include all relevant information within the prompt.</p>
<blockquote>
<p><em>Emphasizing simplicity and precision, and with a well-structured context, prompts in this scenario are analogous to CLI commands expressed in human language (Human Language Interface).</em></p>
</blockquote>
<p>While numerous strategies exist for effective prompt engineering, we aim to engage in straightforward English dialogue <em>without</em> relying on advanced prompt engineering yet still execute precise API calls to the APIC.</p>
<p>Consequently, a basic understanding of networking is necessary to provide ChatGPT with the required parameters during the prompting process</p>
<p>Below are some example prompts. Note that here, we used ACI-specific terminology such as &ldquo;fabric,&rdquo; &ldquo;UR (unicast routing),&rdquo; and &ldquo;BD (Bridge Domain)&rdquo; to assess ChatGPT&rsquo;s awareness of context.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span># This question intends to get the fabric health score
</span></span><span style="display:flex;"><span>- How is my fabric doing?
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span># The intention is to get a list of routed BDs, including the default ones.
</span></span><span style="display:flex;"><span># Excluding default BDs requires further instruction tuning, though
</span></span><span style="display:flex;"><span>- Can you get me a list of BDs with UR enabled?
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span># We&#39;ll further analyze this one in the following article
</span></span><span style="display:flex;"><span># Can you spot any potential issues in this prompt, though?
</span></span><span style="display:flex;"><span>- Can you add a new BD named VLAN5_BD to Tenant customera?
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span># A simple question to count BDs. 
</span></span><span style="display:flex;"><span># We&#39;ll further analyze ChatGPT&#39;s response in the following article
</span></span><span style="display:flex;"><span>- How many BDs are there?
</span></span></code></pre></div><h2 id="bring-everything-together">Bring Everything Together<a hidden class="anchor" aria-hidden="true" href="#bring-everything-together">#</a></h2>
<p>Now that we&rsquo;ve covered all the fundamentals, let&rsquo;s see how to integrate these elements sequentially to create a streamlined process.</p>
<p><img loading="lazy" src="sequence-flow.png" alt="sequence-flow"  />
</p>
<p>Once we receive a prompt from the user, the first step involves sending the following information to the ChatGPT API.</p>
<ul>
<li>Global instructions</li>
<li>List of functions</li>
<li>User prompts</li>
</ul>
<p>This results in the API returning the name of the function we should call and the necessary arguments. We then use this information to execute the appropriate API call to ACI, which yields a JSON response.</p>
<p>At this juncture, we have two options:</p>
<ol>
<li>
<p>We can send the JSON response back to ChatGPT for parsing, as illustrated above. We&rsquo;ll use this approach for all future demonstrations.</p>
</li>
<li>
<p>Alternatively, we can parse the JSON using another function. This approach is less token-intensive but requires more significant software engineering effort.</p>
</li>
</ol>
<p>Finally, we present the output to the user.</p>
<h2 id="a-simple-demonstration">A Simple Demonstration<a hidden class="anchor" aria-hidden="true" href="#a-simple-demonstration">#</a></h2>
<p>Now that we understand the entire process flow let&rsquo;s walk through the first query as a simple demonstration. (we&rsquo;ll go into depth in the next article)</p>
<blockquote>
<p>Q: How is my fabric doing?</p>
</blockquote>
<p>Recall that the intention is to simply get the health score (as defined in the function definition).</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>How can I assist you today? =&gt; how&#39;s my fabric doing
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Making function call....  {
</span></span><span style="display:flex;"><span>  &#34;name&#34;: &#34;get_fabric_health&#34;,
</span></span><span style="display:flex;"><span>  &#34;arguments&#34;: &#34;{}&#34;
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><ul>
<li>
<p>In this instance, specifying the entire &ldquo;Cisco ACI Fabric&rdquo; context wasn&rsquo;t necessary.</p>
</li>
<li>
<p>The function <code>get_fabric_health</code> is correctly returned from ChatGPT with no arguments.</p>
</li>
</ul>
<p>We received the below output after sending the above API call to ACI.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>    &#34;fabricOverallHealthHist5min&#34;: {
</span></span><span style="display:flex;"><span>        &#34;attributes&#34;: {
</span></span><span style="display:flex;"><span>            &#34;childAction&#34;: &#34;&#34;,
</span></span><span style="display:flex;"><span>            &#34;cnt&#34;: &#34;30&#34;,
</span></span><span style="display:flex;"><span>            &#34;dn&#34;: &#34;topology/HDfabricOverallHealth5min-0&#34;,
</span></span><span style="display:flex;"><span>            &#34;healthAvg&#34;: &#34;81&#34;,
</span></span><span style="display:flex;"><span>            &#34;healthMax&#34;: &#34;81&#34;,
</span></span><span style="display:flex;"><span>            &#34;healthMin&#34;: &#34;81&#34;,
</span></span><span style="display:flex;"><span>            &#34;healthSpct&#34;: &#34;0&#34;,
</span></span><span style="display:flex;"><span>            &#34;healthThr&#34;: &#34;&#34;,
</span></span><span style="display:flex;"><span>            &#34;healthTr&#34;: &#34;0&#34;,
</span></span><span style="display:flex;"><span>            &#34;index&#34;: &#34;0&#34;,
</span></span><span style="display:flex;"><span>            &#34;lastCollOffset&#34;: &#34;300&#34;,
</span></span><span style="display:flex;"><span>            &#34;repIntvEnd&#34;: &#34;2023-10-17T19:43:56.439+00:00&#34;,
</span></span><span style="display:flex;"><span>            &#34;repIntvStart&#34;: &#34;2023-10-17T19:38:55.934+00:00&#34;,
</span></span><span style="display:flex;"><span>            &#34;status&#34;: &#34;&#34;
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>The JSON output is subsequently sent to ChatGPT for additional data extraction. Following this, ChatGPT provided a response that precisely captured the requested average, minimum, and maximum values despite them being identical</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>The fabric health for the past 5 minutes has been consistent with an average, 
</span></span><span style="display:flex;"><span>minimum, and maximum health score of 81.
</span></span></code></pre></div><h2 id="variations-inprompts">Variations in Prompts<a hidden class="anchor" aria-hidden="true" href="#variations-inprompts">#</a></h2>
<p>Let&rsquo;s examine how ChatGPT responds to slight variations in phrasing the exact prompt.</p>
<p>Example Prompt: <code>what's my health score?</code> and <code>give me the hs</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>How can I assist you today? =&gt; what&#39;s my health score ?
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>==Response==
</span></span><span style="display:flex;"><span>Making function call....  {
</span></span><span style="display:flex;"><span>  &#34;name&#34;: &#34;get_fabric_health&#34;,
</span></span><span style="display:flex;"><span>  &#34;arguments&#34;: &#34;{}&#34;
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>Your health score for the past 5 minutes is as follows:
</span></span><span style="display:flex;"><span>- Minimum: 84
</span></span><span style="display:flex;"><span>- Maximum: 84
</span></span><span style="display:flex;"><span>- Average: 84
</span></span><span style="display:flex;"><span>==End of response==
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>How can I assist you today? =&gt; give me the hs
</span></span><span style="display:flex;"><span>==Response==
</span></span><span style="display:flex;"><span>Making function call....  {
</span></span><span style="display:flex;"><span>  &#34;name&#34;: &#34;get_fabric_health&#34;,
</span></span><span style="display:flex;"><span>  &#34;arguments&#34;: &#34;{}&#34;
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>Your health score for the past 5 minutes is as follows:
</span></span><span style="display:flex;"><span>- Minimum: 84
</span></span><span style="display:flex;"><span>- Maximum: 84
</span></span><span style="display:flex;"><span>- Average: 84
</span></span><span style="display:flex;"><span>==End of response==
</span></span></code></pre></div><ul>
<li>
<p>In both cases, the response outputs differed from the previous instance. This is a common scenario where asking ChatGPT the same question multiple times can result in slightly different outputs, though they convey the same meaning. In this context, the variation isn&rsquo;t a significant issue. With some additional fine-tuning of the instructions, we can address this.</p>
</li>
<li>
<p>Remarkably, ChatGPT correctly interpreted <code>hs</code> as <code>health score</code>. <em><strong>A cautionary note</strong></em>: only four functions were provided in this example, which probably helped control hallucinations.</p>
</li>
</ul>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>ChatGPT demonstrates proficiency in executing API actions when provided with the correct context, clear guidance, and precise function definitions.</p>
<p>However, achieving output consistency comparable to tools like Ansible requires further refinement of instructions and consideration of user experience.</p>
<p>In the following article, we&rsquo;ll analyze ChatGPT&rsquo;s responses to the remaining questions in depth, examining its capability to handle more complex scenarios.</p>
<p>See you in the next one! <a href="http://zhangineer.net/2023/11/22/Networking-and-LLM-pt3.html">Networking and LLM in the Age of AI - Pt III: In-Depth Analysis of ChatGPT&rsquo;s Responses</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>

<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Networking and LLM in the Age of AI — Pt. II: Exploring Basic Interactions on x"
            href="https://x.com/intent/tweet/?text=Networking%20and%20LLM%20in%20the%20Age%20of%20AI%20%e2%80%94%20Pt.%20II%3a%20Exploring%20Basic%20Interactions&amp;url=http%3a%2f%2fzhangineer.net%2fposts%2f2023-10-29-networking-and-llm-pt2%2f&amp;hashtags=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Networking and LLM in the Age of AI — Pt. II: Exploring Basic Interactions on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2fzhangineer.net%2fposts%2f2023-10-29-networking-and-llm-pt2%2f&amp;title=Networking%20and%20LLM%20in%20the%20Age%20of%20AI%20%e2%80%94%20Pt.%20II%3a%20Exploring%20Basic%20Interactions&amp;summary=Networking%20and%20LLM%20in%20the%20Age%20of%20AI%20%e2%80%94%20Pt.%20II%3a%20Exploring%20Basic%20Interactions&amp;source=http%3a%2f%2fzhangineer.net%2fposts%2f2023-10-29-networking-and-llm-pt2%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Networking and LLM in the Age of AI — Pt. II: Exploring Basic Interactions on reddit"
            href="https://reddit.com/submit?url=http%3a%2f%2fzhangineer.net%2fposts%2f2023-10-29-networking-and-llm-pt2%2f&title=Networking%20and%20LLM%20in%20the%20Age%20of%20AI%20%e2%80%94%20Pt.%20II%3a%20Exploring%20Basic%20Interactions">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Networking and LLM in the Age of AI — Pt. II: Exploring Basic Interactions on facebook"
            href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2fzhangineer.net%2fposts%2f2023-10-29-networking-and-llm-pt2%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Networking and LLM in the Age of AI — Pt. II: Exploring Basic Interactions on whatsapp"
            href="https://api.whatsapp.com/send?text=Networking%20and%20LLM%20in%20the%20Age%20of%20AI%20%e2%80%94%20Pt.%20II%3a%20Exploring%20Basic%20Interactions%20-%20http%3a%2f%2fzhangineer.net%2fposts%2f2023-10-29-networking-and-llm-pt2%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Networking and LLM in the Age of AI — Pt. II: Exploring Basic Interactions on telegram"
            href="https://telegram.me/share/url?text=Networking%20and%20LLM%20in%20the%20Age%20of%20AI%20%e2%80%94%20Pt.%20II%3a%20Exploring%20Basic%20Interactions&amp;url=http%3a%2f%2fzhangineer.net%2fposts%2f2023-10-29-networking-and-llm-pt2%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Networking and LLM in the Age of AI — Pt. II: Exploring Basic Interactions on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Networking%20and%20LLM%20in%20the%20Age%20of%20AI%20%e2%80%94%20Pt.%20II%3a%20Exploring%20Basic%20Interactions&u=http%3a%2f%2fzhangineer.net%2fposts%2f2023-10-29-networking-and-llm-pt2%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="http://zhangineer.net/">Blogs on AI and Networking</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a><script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.6/medium-zoom.min.js" integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>

<script>
const images = Array.from(document.querySelectorAll(".post-content img"));
images.forEach(img => {
  mediumZoom(img, {
    margin: 0,  
    scrollOffset: 40,  
    container: null,  
    template: null  
  });
});
</script>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
